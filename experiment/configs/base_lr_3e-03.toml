train_epochs = 40000
eval_epochs = 1000
batch_size = 32
dataset_name = "TinyStoriesV2-GPT4"
name = "base_lr_3e-03"
save_every_n_epochs = 1000

[module_params]
vocab_size = 10000
context_length = 256
d_model = 512
num_layers = 4
num_heads = 16
d_ff = 1344
rope_theta = 10000.0

[optimizer_params]
learning_rate = 0.003
weight_decay = 0.01
betas = [ 0.9, 0.999,]
eps = 1e-8

[clipping_params]
max_l2_norm = 0.01
